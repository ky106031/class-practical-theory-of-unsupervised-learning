{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#2025年度バージョン\n"
      ],
      "metadata": {
        "id": "B8_A23O2VJuk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 前準備\n"
      ],
      "metadata": {
        "id": "L-vFvqDPpbhA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "gensimのバージョンアップといったん再起動が必要\n"
      ],
      "metadata": {
        "id": "FMXPRi-zAKzT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade gensim\n",
        "\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "DYgPTO9v_jEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "形態素解析Mecabの設定\n"
      ],
      "metadata": {
        "id": "jEWqcAsBpsJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install mecab mecab-ipadic-utf8\n",
        "!pip install mecab-python3\n",
        "!ln -s  /etc/mecabrc /usr/local/etc/mecabrc"
      ],
      "metadata": {
        "id": "Z2Gu-13HVREU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "青空文庫から著作権の切れたテキストを持ってくる"
      ],
      "metadata": {
        "id": "Pv_IiHBGpxUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "urls = ['https://www.aozora.gr.jp/cards/000148/files/752_14964.html',   # 坊ちゃん\n",
        "        'https://www.aozora.gr.jp/cards/000148/files/773_14560.html',   # こころ\n",
        "        'https://www.aozora.gr.jp/cards/000081/files/456_15050.html',   # 銀河鉄道の夜\n",
        "        'https://www.aozora.gr.jp/cards/000081/files/46605_31178.html', # やまなし\n",
        "        'https://www.aozora.gr.jp/cards/000879/files/127_15260.html',   # 羅生門\n",
        "        'https://www.aozora.gr.jp/cards/000879/files/92_14545.html',    # 蜘蛛の糸\n",
        "        'https://www.aozora.gr.jp/cards/000035/files/1567_14913.html',  # 走れメロス\n",
        "        'https://www.aozora.gr.jp/cards/000296/files/47061_29420.html', # 学問のすすめ\n",
        "       ]\n",
        "\n",
        "htmls = []\n",
        "for url in urls:\n",
        "  response = requests.get(url)\n",
        "  print(response.encoding) # SJISのはずなのにISO-8859-1\n",
        "  response.encoding = response.apparent_encoding # encoding情報の修正\n",
        "  print(response.encoding) # S_JIS\n",
        "  htmlfile = response.text # response.encodingでdecodeしてunicodeの文字列にする\n",
        "  print(htmlfile)\n",
        "  htmls.append(htmlfile)\n"
      ],
      "metadata": {
        "id": "7-tYU89vkvMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "タグなど不要なものを削除する"
      ],
      "metadata": {
        "id": "b7nMlHf2p4wp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import lxml\n",
        "\n",
        "\n",
        "table = str.maketrans({'\\r':'','\\n':'','\\u3000':'', ' ':'', '\\t':''}) #改行と空白文字消去のためのテーブル\n",
        "\n",
        "sentences = []\n",
        "\n",
        "for htmlfile in htmls:\n",
        "  soup = BeautifulSoup(htmlfile, \"lxml\") # html/xmlタグを消す\n",
        "  for rubi in soup.findAll([\"rt\", \"rp\"]): # このタグの中身はルビなのでいらない\n",
        "    rubi.decompose()\n",
        "  for metadata in  soup.findAll(\"div\", attrs={ \"class\": [\"bibliographical_information\", \"notation_notes\"] }):\n",
        "    metadata.decompose()\n",
        "  for metadata in  soup.findAll(\"div\", attrs={ \"id\": \"card\" }):\n",
        "    metadata.decompose()\n",
        "  string = soup.body.get_text().strip()\n",
        "  string = string.translate(table)\n",
        "  sentences.append(string)"
      ],
      "metadata": {
        "id": "dqamlRoclLH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BoWと各種トピックベクトルの生成"
      ],
      "metadata": {
        "id": "AdnGdbVTp9KZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import MeCab\n",
        "import seaborn\n",
        "\n",
        "tagger = MeCab.Tagger(\"-Owakati\")\n",
        "# 各sentence（ドキュメント）に対して分かち書き\n",
        "tokenized_docs = [tagger.parse(sentence).split() for sentence in sentences]\n",
        "\n",
        "# 単語のリスト（辞書）をつくる。IDと語のmapping\n",
        "dic = gensim.corpora.Dictionary(tokenized_docs)\n",
        "\n",
        "# 分かち書きされた各ドキュメントをBoWベクトル（頻度）にする\n",
        "BOW_vecs = [dic.doc2bow(tokenized_doc) for tokenized_doc in tokenized_docs]\n",
        "\n",
        "# lsiを行い、ドキュメントベクトル（topic分布ベクトル）を求める\n",
        "lsimodel = gensim.models.LsiModel(BOW_vecs, id2word=dic, num_topics=5)\n",
        "lsi_vecs = [lsimodel[BOW_vec] for BOW_vec in BOW_vecs]\n",
        "\n",
        "# nmfを行い、ドキュメントベクトル（topic分布ベクトル）を求める\n",
        "nmfmodel = gensim.models.Nmf(BOW_vecs, id2word=dic, num_topics=5)\n",
        "nmf_vecs = [nmfmodel[BOW_vec] for BOW_vec in BOW_vecs]\n",
        "\n",
        "# ldaを行い、ドキュメントベクトル（topic分布ベクトル）を求める\n",
        "ldamodel = gensim.models.LdaModel(BOW_vecs, id2word=dic, num_topics=5, passes=10)\n",
        "lda_vecs = [ldamodel[BOW_vec] for BOW_vec in BOW_vecs]\n",
        "\n",
        "\n",
        "# 各ドキュメントとの類似度を求めるための準備\n",
        "index = gensim.similarities.MatrixSimilarity(BOW_vecs, num_features=len(dic))\n",
        "# 各ドキュメント(BOW_vec)と、準備したindexの各ドキュメントとのコサイン類似度をとる\n",
        "bowsimilarities = [index[BOW_vec] for BOW_vec in BOW_vecs]\n",
        "\n",
        "\n",
        "# 各ドキュメントとの類似度を求めるための準備\n",
        "lsiindex = gensim.similarities.MatrixSimilarity(lsi_vecs, num_features=len(dic))\n",
        "# 各ドキュメント(lsi_vec)と、準備したindexの各ドキュメントとのコサイン類似度をとる\n",
        "lsisimilarities = [lsiindex[lsi_vec] for lsi_vec in lsi_vecs]\n",
        "\n",
        "# 各ドキュメントとの類似度を求めるための準備\n",
        "nmfindex = gensim.similarities.MatrixSimilarity(nmf_vecs, num_features=len(dic))\n",
        "# 各ドキュメント(nmf_vec)と、準備したindexの各ドキュメントとのコサイン類似度をとる\n",
        "nmfsimilarities = [nmfindex[nmf_vec] for nmf_vec in nmf_vecs]\n",
        "\n",
        "# 各ドキュメントとの類似度を求めるための準備\n",
        "ldaindex = gensim.similarities.MatrixSimilarity(lda_vecs, num_features=len(dic))\n",
        "# 各ドキュメント(lda_vec)と、準備したindexの各ドキュメントとのコサイン類似度をとる\n",
        "ldasimilarities = [ldaindex[lda_vec] for lda_vec in lda_vecs]\n"
      ],
      "metadata": {
        "id": "Phwq57IIVYKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ベクトル間の類似度の確認"
      ],
      "metadata": {
        "id": "LbBxJZL7qELH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seaborn.heatmap(bowsimilarities, annot=True, cmap='Blues')"
      ],
      "metadata": {
        "id": "vNqLHLLcWcDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seaborn.heatmap(lsisimilarities, annot=True, cmap='Blues')"
      ],
      "metadata": {
        "id": "gZ5RGjOOWgrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seaborn.heatmap(nmfsimilarities, annot=True, cmap='Blues')"
      ],
      "metadata": {
        "id": "XmfKURjYWkEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seaborn.heatmap(ldasimilarities, annot=True, cmap='Blues')"
      ],
      "metadata": {
        "id": "0KTEag0NWo8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BoWをTFIDFにして各種トピックベクトルの生成"
      ],
      "metadata": {
        "id": "l-8CUHkjuYp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import MeCab\n",
        "import seaborn\n",
        "\n",
        "tagger = MeCab.Tagger(\"-Owakati\")\n",
        "# 各sentence（ドキュメント）に対して分かち書き\n",
        "tokenized_docs = [tagger.parse(sentence).split() for sentence in sentences]\n",
        "\n",
        "# 単語のリスト（辞書）をつくる。IDと語のmapping\n",
        "dic = gensim.corpora.Dictionary(tokenized_docs)\n",
        "\n",
        "# 分かち書きされた各ドキュメントをBoWベクトル（頻度）にする\n",
        "BOW_vecs = [dic.doc2bow(tokenized_doc) for tokenized_doc in tokenized_docs]\n",
        "\n",
        "# tfidfを計算し、BoWベクトル（tfidf）を求める\n",
        "tfidfmodel = gensim.models.TfidfModel(BOW_vecs)\n",
        "tfidf_vecs = [tfidfmodel[BOW_vec] for BOW_vec in BOW_vecs]\n",
        "\n",
        "# lsiを行い、ドキュメントベクトル（topic分布ベクトル）を求める\n",
        "lsimodel = gensim.models.LsiModel(tfidf_vecs, id2word=dic, num_topics=5)\n",
        "lsi_vecs = [lsimodel[tfidf_vec] for tfidf_vec in tfidf_vecs]\n",
        "\n",
        "# nmfを行い、ドキュメントベクトル（topic分布ベクトル）を求める\n",
        "nmfmodel = gensim.models.Nmf(tfidf_vecs, id2word=dic, num_topics=5)\n",
        "nmf_vecs = [nmfmodel[tfidf_vec] for tfidf_vec in tfidf_vecs]\n",
        "\n",
        "# ldaを行い、ドキュメントベクトル（topic分布ベクトル）を求める\n",
        "ldamodel = gensim.models.LdaModel(tfidf_vecs, id2word=dic, num_topics=5, passes=10)\n",
        "lda_vecs = [ldamodel[tfidf_vec] for tfidf_vec in tfidf_vecs]\n",
        "\n",
        "\n",
        "# 各ドキュメントとの類似度を求めるための準備\n",
        "tfidfindex = gensim.similarities.MatrixSimilarity(tfidf_vecs, num_features=len(dic))\n",
        "# 各ドキュメント(tfidf_vec)と、準備したindexの各ドキュメントとのコサイン類似度をとる\n",
        "tfidfsimilarities = [tfidfindex[tfidf_vec] for tfidf_vec in tfidf_vecs]\n",
        "\n",
        "# 各ドキュメントとの類似度を求めるための準備\n",
        "lsiindex = gensim.similarities.MatrixSimilarity(lsi_vecs, num_features=len(dic))\n",
        "# 各ドキュメント(lsi_vec)と、準備したindexの各ドキュメントとのコサイン類似度をとる\n",
        "lsisimilarities = [lsiindex[lsi_vec] for lsi_vec in lsi_vecs]\n",
        "\n",
        "# 各ドキュメントとの類似度を求めるための準備\n",
        "nmfindex = gensim.similarities.MatrixSimilarity(nmf_vecs, num_features=len(dic))\n",
        "# 各ドキュメント(nmf_vec)と、準備したindexの各ドキュメントとのコサイン類似度をとる\n",
        "nmfsimilarities = [nmfindex[nmf_vec] for nmf_vec in nmf_vecs]\n",
        "\n",
        "# 各ドキュメントとの類似度を求めるための準備\n",
        "ldaindex = gensim.similarities.MatrixSimilarity(lda_vecs, num_features=len(dic))\n",
        "# 各ドキュメント(lda_vec)と、準備したindexの各ドキュメントとのコサイン類似度をとる\n",
        "ldasimilarities = [ldaindex[lda_vec] for lda_vec in lda_vecs]"
      ],
      "metadata": {
        "id": "A5Wp1IMUuXKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ベクトル間の類似度の確認"
      ],
      "metadata": {
        "id": "ftVlx5EkqSLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seaborn.heatmap(tfidfsimilarities, annot=True, cmap='Blues')"
      ],
      "metadata": {
        "id": "jPvs3d0iuozl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seaborn.heatmap(lsisimilarities, annot=True, cmap='Blues')"
      ],
      "metadata": {
        "id": "I6pPLEchuret"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seaborn.heatmap(nmfsimilarities, annot=True, cmap='Blues')"
      ],
      "metadata": {
        "id": "Yvz5KZBJuvKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seaborn.heatmap(ldasimilarities, annot=True, cmap='Blues')"
      ],
      "metadata": {
        "id": "QOPUvjrVuxXq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}